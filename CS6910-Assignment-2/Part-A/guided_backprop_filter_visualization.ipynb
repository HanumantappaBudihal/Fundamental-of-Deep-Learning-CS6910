{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc27f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# from keras.backend import image_data_format\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "# using mixed_precision speeds up training on GPUs with compute-compatibility >= 8.0\n",
    "# it also halves the memory usage as keras intelligently switched between 16-bit and 32-bit precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some fixed parameters\n",
    "# this code assumes that the dataset is in the same directory as the script\n",
    "# it also assumes that the best model saved file is in the same directory with the name \"best_model.h5\"\n",
    "IMG_SIZE = (300, 300)\n",
    "BASE_PATH = \"./inaturalist_12K\"\n",
    "names = [\"Amphibia\", \"Animalia\", \"Arachnida\", \"Aves\", \"Fungi\", \"Insecta\", \"Mammalia\", \"Mollusca\", \"Plantae\", \"Reptilia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d052609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEST PARAMETERS ###\n",
    "NUM_FILTERS = 32\n",
    "CONV_FILTER_SIZE = 5\n",
    "FILTER_MULT = 2\n",
    "DATA_AUGMENTATION = False\n",
    "DROPOUT = 0.2\n",
    "BATCHNORM = False\n",
    "OPTIM_NAME = \"adamax\"\n",
    "LR = 7e-5\n",
    "BATCH_SIZE = 32\n",
    "DENSE_UNITS = 256\n",
    "DENSE_ACTIVATION = \"relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for reading data from the given directory structure\n",
    "# we rescale our image pixel values by 1/255 but this time we use the whole training data to get the train-test metrics\n",
    "# returns the train and test generators which can be passed to the model.evaluate() method\n",
    "def get_data_generators(data_augmentation=True, img_size=IMG_SIZE, batch_size=32):\n",
    "    if data_augmentation:\n",
    "        # the following augmentation techniques are used\n",
    "        train_data = ImageDataGenerator(rescale=1/255,\n",
    "                                        samplewise_center=True,\n",
    "                                        samplewise_std_normalization=True,\n",
    "                                        shear_range=0.25,\n",
    "                                        zoom_range=[0.25, 1.25],\n",
    "                                        width_shift_range=0.25,\n",
    "                                        height_shift_range=0.25,\n",
    "                                        horizontal_flip=True,\n",
    "                                        rotation_range=60)\n",
    "    else:\n",
    "        train_data = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "    train_gen = train_data.flow_from_directory(f\"{BASE_PATH}/train\",\n",
    "                                         target_size=img_size,\n",
    "                                         batch_size=batch_size,\n",
    "                                         color_mode=\"rgb\",\n",
    "                                         class_mode=\"sparse\",\n",
    "                                         shuffle=True,\n",
    "                                         seed=123,\n",
    "                                         subset=\"training\")\n",
    "    # this time read the test-data as well\n",
    "    # but for evaluation we do not do any augmentation on the test data                       \n",
    "    test_data = ImageDataGenerator(rescale=1/255)\n",
    "    test_gen = test_data.flow_from_directory(f\"{BASE_PATH}/val\",\n",
    "                                             target_size=img_size,\n",
    "                                             batch_size=batch_size,\n",
    "                                             color_mode=\"rgb\",\n",
    "                                             batch_size=1,\n",
    "                                             seed=123,\n",
    "                                             class_mode=\"sparse\",\n",
    "                                             shuffle=True)\n",
    "\n",
    "    return train_gen, test_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014690fe",
   "metadata": {},
   "source": [
    "# 4a) Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f7a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the train, validation and test data\n",
    "train_gen, test_gen = get_data_generators(data_augmentation=DATA_AUGMENTATION, \n",
    "                                          batch_size=BATCH_SIZE)    \n",
    "# load the saved model       \n",
    "model = load_model(\"best_model.h5\")\n",
    "# evaluate the model\n",
    "model.evaluate(train_gen)\n",
    "model.evaluate(test_gen)\n",
    "# visualize the layers of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae38c6",
   "metadata": {},
   "source": [
    "# 4b) Visualize Predictions of the Model\n",
    "### Disclaimer: From now on, images in report might differ from the ones when you run the code, since random images are chosen during every run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788be830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect 30 images, 3 for each class\n",
    "images, c = [[], [], [], [], [], [], [], [], [], []], 0\n",
    "while c < 30:\n",
    "    img, label = test_gen.next()\n",
    "    label = int(label)\n",
    "    if len(images[label]) < 3:\n",
    "        images[label].append(img)\n",
    "        c += 1\n",
    "\n",
    "# plot the images as a 10x3 grid with title and ylabel\n",
    "plt.figure(figsize=(20, 60))\n",
    "images = np.squeeze(np.array(images), axis=2)\n",
    "for i in range(30):\n",
    "    ax = plt.subplot(10, 3, i+1)\n",
    "    x = images[i//3][i % 3]\n",
    "    # predict the class of the input image\n",
    "    y = np.argmax(model.predict(np.array([x])))\n",
    "    # set the ylabel as the true class label in blue\n",
    "    plt.ylabel(names[i//3], color=\"blue\", fontdict={'fontsize': 15, 'fontweight': 'bold'})\n",
    "    color = \"red\" if names[i//3] != names[y] else \"green\"\n",
    "    # set the title as the prediction by the model\n",
    "    # the title is red if the prediction is incorrect else green\n",
    "    plt.title(names[y], color=color, fontdict={'fontsize': 15, 'fontweight': 'bold'})\n",
    "    plt.imshow(x)\n",
    "# save the plot and manually upload to wandb\n",
    "plt.savefig(\"prediction_table\", dpi=300, bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5524c9",
   "metadata": {},
   "source": [
    "# 4c) Visualize Filters and Feature-Maps of First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random image for plotting filters and feature-maps\n",
    "# plot the original image\n",
    "image, label = test_gen.next()\n",
    "image, label = np.squeeze(image, axis=0), int(label)\n",
    "temp_model = Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "plt.title(f\"{names[label]}\", fontdict={'fontsize': 15, 'fontweight': 'bold'})\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.savefig(\"feature_maps_filters_original\", dpi=360, bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "# get the filters\n",
    "filters, _ = model.layers[0].get_weights()\n",
    "n_filters = filters.shape[-1]\n",
    "cols, rows = 8, n_filters//8\n",
    "# scale the filter values\n",
    "filters = (filters - filters.min()) / (filters.max() - filters.min())\n",
    "\n",
    "# plot the filters in a grid\n",
    "plt.figure(figsize=(30, 10))\n",
    "for i in range(n_filters):\n",
    "    ax = plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(filters[:, :, :, i])\n",
    "    plt.title(f\"visualization of filter_{i+1}\", fontdict={'fontsize': 10, 'fontweight': 'bold'})\n",
    "    plt.axis(\"off\")\n",
    "plt.savefig(\"filters_visualization\", dpi=360, bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "# get the feature maps as the output of the first layer\n",
    "# plot the feature maps as a grid\n",
    "feature_maps = temp_model.predict(np.array([image]))\n",
    "plt.figure(figsize=(30, 10))\n",
    "for i in range(n_filters):\n",
    "    ax = plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(feature_maps[0,:,:,i])\n",
    "    plt.title(f\"visualization of feature_map_{i+1}\", fontdict={'fontsize': 10, 'fontweight': 'bold'})\n",
    "    plt.axis(\"off\")\n",
    "plt.savefig(\"feature_map_visualization\", dpi=360, bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8147bb",
   "metadata": {},
   "source": [
    "# 5) Guided Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required functions for guided backprop\n",
    "# define a custom derivative of relu as discussed in class\n",
    "@tf.custom_gradient\n",
    "def guidedRelu(input):\n",
    "    def grad(upstream):\n",
    "        return tf.cast(upstream > 0, \"float32\") * tf.cast(input > 0, \"float32\") * upstream\n",
    "    return tf.nn.relu(input), grad\n",
    "\n",
    "# a function convert the gradient values into a proper image\n",
    "# for this, standardize the values, clip them between [0, 1] and multiply by 255 to get proper pixel values\n",
    "def deprocess_image(x):\n",
    "    x = (x - x.mean())/(x.std() + 1e-8)\n",
    "    x *= 0.25\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "    x *= 255\n",
    "\n",
    "    # if image_data_format() == \"channels_first\":\n",
    "    #     x = x.transpose((1, 2, 0))\n",
    "    return x.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a guided backprop model\n",
    "guided_backprop_model = Model(inputs = [model.inputs], outputs = [model.get_layer(\"conv2d_4\").output])\n",
    "# find the layers which have activation\n",
    "layer_dict = [layer for layer in guided_backprop_model.layers[1:] if hasattr(layer, \"activation\")]\n",
    "out_shape = model.get_layer(\"conv2d_4\").output.shape[1:]\n",
    "\n",
    "# choose a random image and plot it\n",
    "image, label = test_gen.next()\n",
    "image, label = np.squeeze(image, axis=0), int(label)\n",
    "plt.title(f\"{names[label]}\", fontdict={'fontsize': 15, 'fontweight': 'bold'})\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.savefig(\"guided_backprop_original\", dpi=360, bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "# if the activation in the layers we found was relu\n",
    "# change it to guidedRelu to guide the gradients to specific locations\n",
    "for layer in layer_dict:\n",
    "    if layer.activation == relu:\n",
    "        layer.activation = guidedRelu\n",
    "\n",
    "imgs = []\n",
    "for i in range(10):\n",
    "    # choose a random neuron from fliters and apply guided backprop to it\n",
    "    # like this, do it for 10 random neurons\n",
    "    neuron = [np.random.randint(0, j-1) for j in out_shape]\n",
    "    mask = np.zeros(out_shape)\n",
    "    mask[neuron[0], neuron[1], neuron[2]] = 1\n",
    "    # keep track of the gradients with GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        inputs = tf.cast([image], tf.float32)\n",
    "        tape.watch(inputs)\n",
    "        outputs = guided_backprop_model(inputs) * mask\n",
    "    # at the end, convert them from tensors to numpy arrays for plotting\n",
    "    gradients = tape.gradient(outputs, inputs).numpy()[0]\n",
    "    # keep track of which neuron produced what gradients\n",
    "    imgs.append((neuron, gradients))\n",
    "\n",
    "# plot the guided backprop images for those 10 random neurons\n",
    "plt.figure(figsize=(60, 60))\n",
    "for i, (neuron, gradients) in enumerate(imgs):\n",
    "    ax = plt.subplot(10, 1, i+1)\n",
    "    plt.imshow(deprocess_image(gradients))\n",
    "    plt.title(f\"guided_backprop for neuron-{neuron}\", fontdict={'fontsize': 12, 'fontweight': 'bold'})\n",
    "    plt.axis(\"off\")\n",
    "plt.savefig(\"guided_backprop_visualization\", dpi=360, bbox_inches=\"tight\", pad_inches=0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5abfe2962b098e80363c331fe2ecf3355a4d8fd7c3024fc347af1618375657d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
