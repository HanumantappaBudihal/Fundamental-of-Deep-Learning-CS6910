{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f179328b",
      "metadata": {
        "id": "f179328b"
      },
      "outputs": [],
      "source": [
        "# import statements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from VanillaModels import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow_addons.optimizers import RectifiedAdam, Lookahead, NovoGrad, SGDW, AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "04bf547c",
      "metadata": {
        "id": "04bf547c"
      },
      "outputs": [],
      "source": [
        "### BEST PARAMETERS ###\n",
        "LAYER_TYPE = \"lstm\"\n",
        "NUM_RECURRENT_UNITS = 256\n",
        "ENC_EMBED_DIM = 64\n",
        "DEC_EMBED_DIM = 256\n",
        "DROPOUT = 0.2\n",
        "NUM_ENCODER_RECURRENT_LAYERS = 2\n",
        "NUM_DECODER_RECURRENT_LAYERS = 3\n",
        "OPTIMIZER = \"adamw\"\n",
        "LR = 0.01\n",
        "WEIGHT_DECAY = 0.001\n",
        "BATCH_SIZE = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5619fb82",
      "metadata": {
        "id": "5619fb82"
      },
      "outputs": [],
      "source": [
        "# directory paths\n",
        "train_dir = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "dev_dir = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
        "test_dir = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b402f338",
      "metadata": {
        "id": "b402f338"
      },
      "outputs": [],
      "source": [
        "# define a function to compute word-level accuracy\n",
        "# after stripping of all pad-tokens\n",
        "def compute_word_accuracy(y_true, y_pred, tokens):\n",
        "    # count to keep track of correct predictions\n",
        "    # and complete set of predictions and targets\n",
        "    count, S_y, S_t = 0, [], []\n",
        "\n",
        "    for t, y in zip(y_true, y_pred):\n",
        "        # s_t and s_y are the target and prediction\n",
        "        s_y, s_t = '', ''\n",
        "        for i in y:\n",
        "            c = tokens[int(i)]\n",
        "            # if we encounter stop-token, stop forming the word\n",
        "            if c == '>':\n",
        "                break \n",
        "            # else add the character to the string\n",
        "            s_y += c\n",
        "        # strip all unnecessary characters and append to set of all predictions\n",
        "        s_y = s_y.strip()\n",
        "        S_y.append(s_y)\n",
        "        for i in t:\n",
        "            c = tokens[int(i)]\n",
        "            # if we encounter stop-token, stop forming the word\n",
        "            if c == '>':\n",
        "                break \n",
        "            # else add the character to the string\n",
        "            s_t += c\n",
        "        # strip all unnecessary characters and append to set of all predictions\n",
        "        s_t = s_t.strip()\n",
        "        S_t.append(s_t)\n",
        "        # check if the target word == predicted word\n",
        "        count += int(s_t == s_y)\n",
        "\n",
        "    # create a dataframe from all the targets and predictions\n",
        "    df = pd.DataFrame(list(zip(S_t, S_y)), columns=['Target', 'Prediction']) \n",
        "    # to compute accuracy, divide by total number of items in the dataset\n",
        "    # return both accuracy and dataframe\n",
        "    return count/len(y_true), df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ecad1e8f",
      "metadata": {
        "id": "ecad1e8f"
      },
      "outputs": [],
      "source": [
        "# a function to read the data into a pd dataframe\n",
        "def load_data(path):\n",
        "    data = pd.read_csv(path, \n",
        "                       sep='\\t',\n",
        "                       encoding=\"utf8\",\n",
        "                       names=[\"hi\",\"en\",\"_\"], \n",
        "                       skip_blank_lines=True)\n",
        "                           \n",
        "    data = data[data['hi'].notna()]\n",
        "    data = data[data['en'].notna()]\n",
        "    data = data[['hi','en']]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d7da6236",
      "metadata": {
        "id": "d7da6236"
      },
      "outputs": [],
      "source": [
        "# a function to preprocess the data\n",
        "def pre_process(data, max_eng_len, max_hin_len, eng_token_map, hin_token_map):\n",
        "    x = data['en'].values \n",
        "    # add start and end tokens to the hindi word\n",
        "    y = '<' + data['hi'].values + '>'\n",
        "    \n",
        "    # a is the encoder input\n",
        "    a = np.zeros((len(x), max_eng_len))\n",
        "    # b is the decoder input (has start-token and end-token)\n",
        "    b = np.zeros((len(y), max_hin_len))\n",
        "    # c is the decoder output, which leads the decoder input by one step\n",
        "    # as it does not have start token in the beginning\n",
        "    c = np.zeros((len(y), max_hin_len))\n",
        "    \n",
        "    # replace the characters by numbers so that the model can process them\n",
        "    # use a inverted_index to map the characters to integers\n",
        "    # these integers are just the index when the vocabulary characters are sorted\n",
        "    for i, (xx, yy) in enumerate(zip(x, y)):\n",
        "        for j, ch in enumerate(xx):\n",
        "            a[i, j] = eng_token_map[ch]\n",
        "        for j, ch in enumerate(yy):\n",
        "            b[i, j] = hin_token_map[ch]\n",
        "            if j > 0:\n",
        "                c[i, j-1] = hin_token_map[ch]\n",
        "    return a, b, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9ce6fd7c",
      "metadata": {
        "id": "9ce6fd7c"
      },
      "outputs": [],
      "source": [
        "# load the train, validation and test data\n",
        "train = load_data(train_dir)\n",
        "dev = load_data(dev_dir)\n",
        "test = load_data(test_dir)\n",
        "\n",
        "# add start and end tokens to the hindi word\n",
        "# now generate the english and hindi vocabulary\n",
        "x = train['en'].values\n",
        "y = '<' + train['hi'].values + '>'\n",
        "\n",
        "# get the set of all unique characters, i.e. the vocabulary\n",
        "eng_tokens = set()\n",
        "hin_tokens = set()\n",
        "for xx, yy in zip(x,y):\n",
        "    for ch in xx:\n",
        "        eng_tokens.add(ch)\n",
        "    for ch in yy:\n",
        "        hin_tokens.add(ch)\n",
        "\n",
        "# sort the characters and create a inverted_index \n",
        "# to map the characters to their index in the vocabulary\n",
        "eng_tokens = sorted(list(eng_tokens))\n",
        "hin_tokens = sorted(list(hin_tokens))\n",
        "eng_token_map = dict([(ch, i+1) for i, ch in enumerate(eng_tokens)])\n",
        "hin_token_map = dict([(ch, i+1) for i, ch in enumerate(hin_tokens)])\n",
        "eng_tokens.insert(0, ' ')\n",
        "hin_tokens.insert(0, ' ')\n",
        "eng_token_map[' '] = 0\n",
        "hin_token_map[' '] = 0\n",
        "max_eng_len = max([len(xx) for xx in x])\n",
        "max_hin_len = max([len(yy) for yy in y])\n",
        "\n",
        "# get the training encoder input, decoder input and decoder target\n",
        "trainxe, trainxd, trainy = pre_process(train, \n",
        "                                       max_eng_len, \n",
        "                                       max_hin_len, \n",
        "                                       eng_token_map, \n",
        "                                       hin_token_map)\n",
        "\n",
        "# get the validation encoder input, decoder input and decoder target\n",
        "valxe, valxd, valy = pre_process(dev, \n",
        "                                 max_eng_len, \n",
        "                                 max_hin_len, \n",
        "                                 eng_token_map, \n",
        "                                 hin_token_map)\n",
        "\n",
        "# get the test encoder input, decoder input and decoder target\n",
        "# ignore the decoder target and only use it to check the metrics at the end\n",
        "testxe, testxd, testy = pre_process(test,\n",
        "                                    max_eng_len, \n",
        "                                    max_hin_len, \n",
        "                                    eng_token_map, \n",
        "                                    hin_token_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a81b6406",
      "metadata": {
        "id": "a81b6406",
        "outputId": "00d30d57-b5f9-46ad-d689-0d2dce27d67e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "173/173 [==============================] - 262s 592ms/step - loss: 2.5466 - accuracy: 0.7589 - val_loss: 2.2223 - val_accuracy: 0.7278\n",
            "Epoch 2/25\n",
            "173/173 [==============================] - 50s 289ms/step - loss: 0.8882 - accuracy: 0.8889 - val_loss: 1.1551 - val_accuracy: 0.8496\n",
            "Epoch 3/25\n",
            "173/173 [==============================] - 50s 290ms/step - loss: 0.4183 - accuracy: 0.9364 - val_loss: 0.9637 - val_accuracy: 0.8675\n",
            "Epoch 4/25\n",
            "173/173 [==============================] - 50s 292ms/step - loss: 0.3040 - accuracy: 0.9495 - val_loss: 0.8984 - val_accuracy: 0.8790\n",
            "Epoch 5/25\n",
            "173/173 [==============================] - 50s 292ms/step - loss: 0.2565 - accuracy: 0.9568 - val_loss: 0.8857 - val_accuracy: 0.8800\n",
            "Epoch 6/25\n",
            "173/173 [==============================] - 50s 292ms/step - loss: 0.2302 - accuracy: 0.9608 - val_loss: 0.8772 - val_accuracy: 0.8809\n",
            "Epoch 7/25\n",
            "173/173 [==============================] - 51s 294ms/step - loss: 0.2130 - accuracy: 0.9638 - val_loss: 0.8581 - val_accuracy: 0.8856\n",
            "Epoch 8/25\n",
            "173/173 [==============================] - 51s 293ms/step - loss: 0.2012 - accuracy: 0.9663 - val_loss: 0.8465 - val_accuracy: 0.8854\n",
            "Epoch 9/25\n",
            "173/173 [==============================] - 51s 293ms/step - loss: 0.1916 - accuracy: 0.9684 - val_loss: 0.8596 - val_accuracy: 0.8870\n",
            "Epoch 10/25\n",
            "173/173 [==============================] - 51s 294ms/step - loss: 0.1856 - accuracy: 0.9693 - val_loss: 0.8661 - val_accuracy: 0.8873\n",
            "Epoch 11/25\n",
            "173/173 [==============================] - 51s 296ms/step - loss: 0.1786 - accuracy: 0.9709 - val_loss: 0.8500 - val_accuracy: 0.8872\n",
            "Epoch 12/25\n",
            "173/173 [==============================] - 51s 295ms/step - loss: 0.1747 - accuracy: 0.9718 - val_loss: 0.8645 - val_accuracy: 0.8881\n",
            "Epoch 13/25\n",
            "173/173 [==============================] - 50s 292ms/step - loss: 0.1701 - accuracy: 0.9727 - val_loss: 0.8690 - val_accuracy: 0.8867\n",
            "Epoch 14/25\n",
            "173/173 [==============================] - 50s 287ms/step - loss: 0.1665 - accuracy: 0.9737 - val_loss: 0.8795 - val_accuracy: 0.8867\n",
            "Epoch 15/25\n",
            "173/173 [==============================] - 51s 294ms/step - loss: 0.1637 - accuracy: 0.9741 - val_loss: 0.8724 - val_accuracy: 0.8874\n",
            "Epoch 16/25\n",
            "173/173 [==============================] - 63s 363ms/step - loss: 0.1610 - accuracy: 0.9748 - val_loss: 0.8584 - val_accuracy: 0.8886\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x24ce142fc40>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Since we have custom objects, we can't save the model so easily\n",
        "# Therefore, we have to re-train the model with the test parameters again\n",
        "# create the encoder with the best hyperparameters\n",
        "encoder = Encoder(input_dim=int(trainxe.max())+1,\n",
        "                  embed_dim=ENC_EMBED_DIM,\n",
        "                  cell_hidden_dim=NUM_RECURRENT_UNITS,\n",
        "                  dropout=DROPOUT,\n",
        "                  k=NUM_ENCODER_RECURRENT_LAYERS, \n",
        "                  cell_type=LAYER_TYPE)\n",
        "\n",
        "# create the decoder with the best hyperparameters\n",
        "decoder = Decoder(input_dim=int(trainxd.max())+1, \n",
        "                  output_dim=int(trainy.max())+1, \n",
        "                  embed_dim=DEC_EMBED_DIM,\n",
        "                  cell_hidden_dim=NUM_RECURRENT_UNITS,\n",
        "                  dropout=DROPOUT,\n",
        "                  k=NUM_DECODER_RECURRENT_LAYERS,\n",
        "                  cell_type=LAYER_TYPE)\n",
        "\n",
        "# create the transliteration model with the created encoder and decoder\n",
        "model = TransliterationModel(encoder=encoder, \n",
        "                             decoder=decoder, \n",
        "                             tgt_max_len=max_hin_len)\n",
        "\n",
        "# instantiate and use the best optimizer\n",
        "optimizer = {\n",
        "    \"ranger\": Lookahead(RectifiedAdam(learning_rate=LR, weight_decay=WEIGHT_DECAY, amsgrad=True)),\n",
        "    \"adamw\": AdamW(learning_rate=LR, weight_decay=WEIGHT_DECAY, amsgrad=True),\n",
        "    \"sgdw\": SGDW(learning_rate=LR, weight_decay=WEIGHT_DECAY, momentum=0.9, nesterov=True),\n",
        "    \"novograd\": NovoGrad(learning_rate=LR, weight_decay=WEIGHT_DECAY, amsgrad=True)\n",
        "}[OPTIMIZER]\n",
        "\n",
        "# define early stopping to terminate the run if the validation accuracy drops\n",
        "# continously for 4 times\n",
        "early_stop = EarlyStopping(monitor=\"val_accuracy\",\n",
        "                           patience=4,\n",
        "                           restore_best_weights=True,\n",
        "                           min_delta=1e-3)\n",
        "                           \n",
        "# compile the model and fit it to the data\n",
        "model.compile(optimizer=optimizer, \n",
        "              loss=\"sparse_categorical_crossentropy\", \n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit([trainxe, trainxd], \n",
        "          trainy, \n",
        "          epochs=25, \n",
        "          callbacks=[early_stop],\n",
        "          batch_size=BATCH_SIZE,\n",
        "          validation_data=([valxe, valxd], valy), \n",
        "          shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fbc3cdfa",
      "metadata": {
        "id": "fbc3cdfa",
        "outputId": "af68163b-9b27-4282-fb20-c1fef045808a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test_word_accuracy: 0.3880\n",
            "\n",
            "mosad मोसाद \u001b[91mमोसद\u001b[00m\n",
            "gandaki गंडकी \u001b[92mगंडकी\u001b[00m\n",
            "aarushi आरुषि \u001b[91mआरुषी\u001b[00m\n",
            "raubadar रौबदार \u001b[92mरौबदार\u001b[00m\n",
            "bolane बोलने \u001b[92mबोलने\u001b[00m\n",
            "chhah छह \u001b[91mछाह\u001b[00m\n",
            "urvarata उर्वरता \u001b[92mउर्वरता\u001b[00m\n",
            "rugna रुग्ण \u001b[91mरूगना\u001b[00m\n",
            "piki पिकी \u001b[91mपीकी\u001b[00m\n",
            "domenic डोमेनिक \u001b[92mडोमेनिक\u001b[00m\n"
          ]
        }
      ],
      "source": [
        "# get predictions for the test data\n",
        "# use the encoder input for the encoder and start-tokens for the decoder\n",
        "test_pred = model.predict([testxe, testxd[:, 0]], \n",
        "                          batch_size=BATCH_SIZE)\n",
        "                          \n",
        "# obtain the test word-level accuracy and complete set of predictions\n",
        "test_word_accuracy, df = compute_word_accuracy(testy.tolist(), \n",
        "                                               test_pred.tolist(), \n",
        "                                               hin_tokens)\n",
        "df.insert(loc=0, column=\"data\", value=test['en'])\n",
        "\n",
        "# save the predictions as a csv file\n",
        "print(f\"Test_word_accuracy: {test_word_accuracy:.4f}\\n\")\n",
        "df.to_csv(\"./predictions_vanilla/predictions.csv\", encoding=\"utf-8\")\n",
        "\n",
        "# sample 10 random predictions and display them with color\n",
        "df = df.sample(frac=0.0022).values.tolist()\n",
        "\n",
        "# print the predictions and target with colors\n",
        "# if the prediction is incorrect, it is printed in red\n",
        "# else if it is correct, it is printed in green\n",
        "for (x, y, z) in df:\n",
        "    color = '\\033[91m' if y != z else '\\033[92m'\n",
        "    print(x, y, f\"{color}{z}\\033[00m\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "test_vanilla_seq2seq.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "7a2000831a5050d8503f24d6733c4641a8734e9c81b6dced7c2deb928c6c3201"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
