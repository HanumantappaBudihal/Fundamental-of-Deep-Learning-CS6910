{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f179328b",
      "metadata": {
        "id": "f179328b"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# import statements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from AttentionModels import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow_addons.optimizers import RectifiedAdam, Lookahead, AdamW\n",
        "from random import randint\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04bf547c",
      "metadata": {
        "id": "04bf547c"
      },
      "outputs": [],
      "source": [
        "### BEST PARAMETERS ###\n",
        "LAYER_TYPE = \"lstm\"\n",
        "NUM_RECURRENT_UNITS = 256\n",
        "ENC_EMBED_DIM = 256\n",
        "DEC_EMBED_DIM = 256\n",
        "ATTN_DIM = 256\n",
        "DROPOUT = 0.2\n",
        "NUM_ENCODER_RECURRENT_LAYERS = 1\n",
        "NUM_DECODER_RECURRENT_LAYERS = 3\n",
        "OPTIMIZER = \"adamw\"\n",
        "LR = 0.01\n",
        "WEIGHT_DECAY = 0.0001\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5619fb82",
      "metadata": {
        "id": "5619fb82"
      },
      "outputs": [],
      "source": [
        "# directory paths\n",
        "train_dir = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "dev_dir = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
        "test_dir = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b402f338",
      "metadata": {
        "id": "b402f338"
      },
      "outputs": [],
      "source": [
        "# define a function to compute word-level accuracy\n",
        "# after stripping of all pad-tokens\n",
        "def compute_word_accuracy(y_true, y_pred, tokens):\n",
        "    # count to keep track of correct predictions\n",
        "    # and complete set of predictions and targets\n",
        "    count, S_y, S_t = 0, [], []\n",
        "\n",
        "    for t, y in zip(y_true, y_pred):\n",
        "        # s_t and s_y are the target and prediction\n",
        "        s_y, s_t = '', ''\n",
        "        for i in y:\n",
        "            c = tokens[int(i)]\n",
        "            # if we encounter stop-token, stop forming the word\n",
        "            if c == '>':\n",
        "                break \n",
        "            # else add the character to the string\n",
        "            s_y += c\n",
        "        # strip all unnecessary characters and append to set of all predictions\n",
        "        s_y = s_y.strip()\n",
        "        S_y.append(s_y)\n",
        "        for i in t:\n",
        "            c = tokens[int(i)]\n",
        "            # if we encounter stop-token, stop forming the word\n",
        "            if c == '>':\n",
        "                break \n",
        "            # else add the character to the string\n",
        "            s_t += c\n",
        "        # strip all unnecessary characters and append to set of all predictions\n",
        "        s_t = s_t.strip()\n",
        "        S_t.append(s_t)\n",
        "        # check if the target word == predicted word\n",
        "        count += int(s_t == s_y)\n",
        "\n",
        "    # create a dataframe from all the targets and predictions\n",
        "    df = pd.DataFrame(list(zip(S_t, S_y)), columns=['Target', 'Prediction']) \n",
        "    # to compute accuracy, divide by total number of items in the dataset\n",
        "    # return both accuracy and dataframe\n",
        "    return count/len(y_true), df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecad1e8f",
      "metadata": {
        "id": "ecad1e8f"
      },
      "outputs": [],
      "source": [
        "# a function to read the data into a pd dataframe\n",
        "def load_data(path):\n",
        "    data = pd.read_csv(path, \n",
        "                       sep='\\t',\n",
        "                       encoding=\"utf8\",\n",
        "                       names=[\"hi\",\"en\",\"_\"], \n",
        "                       skip_blank_lines=True)\n",
        "                           \n",
        "    data = data[data['hi'].notna()]\n",
        "    data = data[data['en'].notna()]\n",
        "    data = data[['hi','en']]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7da6236",
      "metadata": {
        "id": "d7da6236"
      },
      "outputs": [],
      "source": [
        "# a function to preprocess the data\n",
        "def pre_process(data, max_eng_len, max_hin_len, eng_token_map, hin_token_map):\n",
        "    x = data['en'].values \n",
        "    # add start and end tokens to the hindi word\n",
        "    y = '<' + data['hi'].values + '>'\n",
        "    \n",
        "    # a is the encoder input\n",
        "    a = np.zeros((len(x), max_eng_len))\n",
        "    # b is the decoder input (has start-token and end-token)\n",
        "    b = np.zeros((len(y), max_hin_len))\n",
        "    # c is the decoder output, which leads the decoder input by one step\n",
        "    # as it does not have start token in the beginning\n",
        "    c = np.zeros((len(y), max_hin_len))\n",
        "    \n",
        "    # replace the characters by numbers so that the model can process them\n",
        "    # use a inverted_index to map the characters to integers\n",
        "    # these integers are just the index when the vocabulary characters are sorted\n",
        "    for i, (xx, yy) in enumerate(zip(x, y)):\n",
        "        for j, ch in enumerate(xx):\n",
        "            a[i, j] = eng_token_map[ch]\n",
        "        for j, ch in enumerate(yy):\n",
        "            b[i, j] = hin_token_map[ch]\n",
        "            if j > 0:\n",
        "                c[i, j-1] = hin_token_map[ch]\n",
        "    return a, b, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ce6fd7c",
      "metadata": {
        "id": "9ce6fd7c"
      },
      "outputs": [],
      "source": [
        "# load the train, validation and test data\n",
        "train = load_data(train_dir)\n",
        "dev = load_data(dev_dir)\n",
        "test = load_data(test_dir)\n",
        "\n",
        "# add start and end tokens to the hindi word\n",
        "# now generate the english and hindi vocabulary\n",
        "x = train['en'].values\n",
        "y = '<' + train['hi'].values + '>'\n",
        "\n",
        "# get the set of all unique characters, i.e. the vocabulary\n",
        "eng_tokens = set()\n",
        "hin_tokens = set()\n",
        "for xx, yy in zip(x,y):\n",
        "    for ch in xx:\n",
        "        eng_tokens.add(ch)\n",
        "    for ch in yy:\n",
        "        hin_tokens.add(ch)\n",
        "\n",
        "# sort the characters and create a inverted_index \n",
        "# to map the characters to their index in the vocabulary\n",
        "eng_tokens = sorted(list(eng_tokens))\n",
        "hin_tokens = sorted(list(hin_tokens))\n",
        "eng_token_map = dict([(ch, i+1) for i, ch in enumerate(eng_tokens)])\n",
        "hin_token_map = dict([(ch, i+1) for i, ch in enumerate(hin_tokens)])\n",
        "eng_tokens.insert(0, ' ')\n",
        "hin_tokens.insert(0, ' ')\n",
        "eng_token_map[' '] = 0\n",
        "hin_token_map[' '] = 0\n",
        "max_eng_len = max([len(xx) for xx in x])\n",
        "max_hin_len = max([len(yy) for yy in y])\n",
        "\n",
        "# get the training encoder input, decoder input and decoder target\n",
        "trainxe, trainxd, trainy = pre_process(train, \n",
        "                                       max_eng_len, \n",
        "                                       max_hin_len, \n",
        "                                       eng_token_map, \n",
        "                                       hin_token_map)\n",
        "\n",
        "# get the validation encoder input, decoder input and decoder target\n",
        "valxe, valxd, valy = pre_process(dev, \n",
        "                                 max_eng_len, \n",
        "                                 max_hin_len, \n",
        "                                 eng_token_map, \n",
        "                                 hin_token_map)\n",
        "\n",
        "# get the test encoder input, decoder input and decoder target\n",
        "# ignore the decoder target and only use it to check the metrics at the end\n",
        "testxe, testxd, testy = pre_process(test,\n",
        "                                    max_eng_len, \n",
        "                                    max_hin_len, \n",
        "                                    eng_token_map, \n",
        "                                    hin_token_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a81b6406",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a81b6406",
        "outputId": "db31505e-2959-4ec4-f502-ad8aa79f0d76"
      },
      "outputs": [],
      "source": [
        "# Since we have custom objects, we can't save the model so easily\n",
        "# Therefore, we have to re-train the model with the test parameters again\n",
        "# create the encoder with the best hyperparameters\n",
        "encoder = Encoder(input_dim=int(trainxe.max())+1,\n",
        "                  embed_dim=ENC_EMBED_DIM,\n",
        "                  cell_hidden_dim=NUM_RECURRENT_UNITS,\n",
        "                  dropout=DROPOUT,\n",
        "                  k=NUM_ENCODER_RECURRENT_LAYERS, \n",
        "                  cell_type=LAYER_TYPE)\n",
        "\n",
        "# create the decoder with the best hyperparameters\n",
        "decoder = AttentionDecoder(input_dim=int(trainxd.max())+1, \n",
        "                           output_dim=int(trainy.max())+1, \n",
        "                           embed_dim=DEC_EMBED_DIM,\n",
        "                           attn_dim=ATTN_DIM,\n",
        "                           cell_hidden_dim=NUM_RECURRENT_UNITS,\n",
        "                           dropout=DROPOUT,\n",
        "                           k=NUM_DECODER_RECURRENT_LAYERS,\n",
        "                           cell_type=LAYER_TYPE)\n",
        "\n",
        "# create the transliteration model with the created encoder and decoder\n",
        "model = TransliterationModel(encoder=encoder, \n",
        "                             decoder=decoder, \n",
        "                             tgt_max_len=max_hin_len)\n",
        "\n",
        "# instantiate and use the best optimizer\n",
        "optimizer = {\n",
        "    \"ranger\": Lookahead(RectifiedAdam(learning_rate=LR, weight_decay=WEIGHT_DECAY, amsgrad=True)),\n",
        "    \"adamw\": AdamW(learning_rate=LR, weight_decay=WEIGHT_DECAY, amsgrad=True),\n",
        "}[OPTIMIZER]\n",
        "\n",
        "# define early stopping to terminate the run if the validation accuracy drops\n",
        "# continously for 4 times\n",
        "early_stop = EarlyStopping(monitor=\"val_accuracy\",\n",
        "                           patience=4,\n",
        "                           restore_best_weights=True,\n",
        "                           min_delta=1e-3)\n",
        "                           \n",
        "# compile the model and fit it to the data\n",
        "model.compile(optimizer=optimizer, \n",
        "              loss=\"sparse_categorical_crossentropy\", \n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit([trainxe, trainxd], \n",
        "          trainy, \n",
        "          epochs=25, \n",
        "          callbacks=[early_stop],\n",
        "          batch_size=BATCH_SIZE,\n",
        "          validation_data=([valxe, valxd], valy), \n",
        "          shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39269c75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39269c75",
        "outputId": "9a198865-628e-408f-e20c-96bef7518f49"
      },
      "outputs": [],
      "source": [
        "# create a tf dataset from the test data to work with batches easily\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((testxe, testxd)).batch(BATCH_SIZE)\n",
        "attention_weights, test_pred = [], []\n",
        "\n",
        "# get the predictions and attention weights for each input batch\n",
        "for xe, xd in test_dataset:\n",
        "    p, a = model([xe, xd[:, 0]])\n",
        "    test_pred.append(p.numpy())\n",
        "    attention_weights.append(a.numpy())\n",
        "\n",
        "# concatenate all predictions into a single list\n",
        "attention_weights = np.concatenate(attention_weights, axis=0)\n",
        "test_pred = np.concatenate(test_pred, axis=0)\n",
        "\n",
        "# obtain the test word-level accuracy and complete set of predictions\n",
        "test_word_accuracy, df = compute_word_accuracy(testy.tolist(), \n",
        "                                               test_pred.tolist(), \n",
        "                                               hin_tokens)\n",
        "\n",
        "# save the predictions as a csv file\n",
        "df.insert(loc=0, column=\"data\", value=test['en'])\n",
        "df.to_csv(\"predictions_attention/predictions.csv\", encoding=\"utf-8\")\n",
        "print(f\"Test_word_accuracy: {test_word_accuracy:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd6cf836",
      "metadata": {},
      "source": [
        "# Plot Attention Heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb8889d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fb8889d9",
        "outputId": "26b83137-97ce-44fe-a6fa-e48b37278d6e"
      },
      "outputs": [],
      "source": [
        "# set required font for devanagiri characters\n",
        "font_prop = FontProperties(fname=\"VesperLibre-Regular.ttf\")\n",
        "attn_maps, xs, zs = [], [], []\n",
        "df = df.values.tolist()\n",
        "\n",
        "correct = 0\n",
        "# sample 9 random english words and their corresponding hindi transliterations and attention weights\n",
        "for _ in range(9):\n",
        "    i = randint(0, len(df)-1)\n",
        "    x, y, z = df[i]\n",
        "    correct += int(y == z)\n",
        "    aw = attention_weights[i]\n",
        "    mp = aw[:len(z)][:, :len(x)]\n",
        "    attn_maps.append(mp)\n",
        "    xs.append(x)\n",
        "    zs.append(z)\n",
        "\n",
        "# check how many of that sample are correct\n",
        "print(f\"correct: {correct}/9\")\n",
        "plt.close('all')\n",
        "# plot those attention weights as a heatmap using sns\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 15), constrained_layout=True)\n",
        "plt.suptitle('Attention Heatmaps', fontsize='xx-large')\n",
        "\n",
        "for x, z, mp, ax in zip(xs, zs, attn_maps, axes.flat):\n",
        "    g = sns.heatmap(mp, linewidth=0.5, ax=ax)\n",
        "    # set necessary fonts and ticks for neat images\n",
        "    g.set_xticklabels(list(x), fontproperties=font_prop, fontsize='xx-large')\n",
        "    g.set_yticklabels(list(z), fontproperties=font_prop, rotation=45, fontsize='xx-large')\n",
        "    g.set_xlabel(f'{x}', fontproperties=font_prop, fontsize=\"xx-large\")\n",
        "    g.set_ylabel(f'{z}', fontproperties=font_prop, fontsize=\"xx-large\")\n",
        "    g.tick_params(labelsize=15)\n",
        "    g.xaxis.tick_top()\n",
        "    g.xaxis.set_label_position('top')\n",
        "    g.set_aspect(\"equal\")\n",
        "    g.set_frame_on(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W98o0xOKWnI2",
      "metadata": {
        "id": "W98o0xOKWnI2"
      },
      "source": [
        "# Visualize the connectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gx6ILIZ8FJxD",
      "metadata": {
        "id": "gx6ILIZ8FJxD"
      },
      "outputs": [],
      "source": [
        "## MOSTLY CODE TAKEN FROM BLOG GIVEN IN THE QUEUSTION ##\n",
        "def get_clr(value):\n",
        "    colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8'\n",
        "              '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
        "              '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
        "              '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
        "    value = min(int((value * 100) / 5), len(colors)-1)\n",
        "    return colors[value]\n",
        "\n",
        "def cstr(s, color='black'):\n",
        "    if s == ' ':\n",
        "        return f\"<text style=color:#000;padding-left:10px;background-color:{s}> </text>\"\n",
        "    return f\"<text style=color:#000;background-color:{color}>{s} </text>\"\n",
        "\n",
        "# helps print colors in html document\n",
        "def print_color(t):\n",
        "    display(HTML(''.join([cstr(t_i, color=c_i) for (t_i, c_i) in t])))\n",
        " \n",
        "# for each character being decoded, highlight the input sequence characters according to the attention weights\n",
        "def visualize(input_word, output_word, attn_map, idx):\n",
        "    print(f\"Highlighting connectivity for: {output_word[idx]}\")\n",
        "    text_colours = [(c, get_clr(a)) for (c, a) in zip(input_word, attn_map[idx])]\n",
        "    print_color(text_colours)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MVy1voXRM5pj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MVy1voXRM5pj",
        "outputId": "d149831b-30cc-44fc-b9e2-3c5ae58ab8f1"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "# sample and print the connectivity for 5 random samples\n",
        "for _ in range(5):\n",
        "    i = randint(0, len(df)-1)\n",
        "    (x, y, z), mp = df[i], attention_weights[i]\n",
        "    # to check how many of those sample are correct\n",
        "    correct += int(y == z)\n",
        "    # plot the visualization\n",
        "    print(f\"visualization for {x} --> {z}\")\n",
        "    for idx in range(len(z)):\n",
        "        mp = mp[:len(z)][:, :len(x)]\n",
        "        visualize(x, z, mp, idx)\n",
        "    print(\"-\"*50)\n",
        "print(f\"correct: {correct}/5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "test_attention_seq2seq.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "7a2000831a5050d8503f24d6733c4641a8734e9c81b6dced7c2deb928c6c3201"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
